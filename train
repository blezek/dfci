#!/bin/env python
# -*- mode: python; -*-

import os
import sys, logging
os.environ['KERAS_BACKEND'] = 'tensorflow'

from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, TensorBoard
import math
import numpy as np
import numpy
import nibabel as nib
import keras
import h5py
import argparse
import glob
import re
import json
import random
import progressbar

np.random.seed(1337) # for reproducibility

from keras.models import Model, Sequential
from keras.layers.core import Dense, Dropout, Flatten
from keras.layers.convolutional import Convolution3D, MaxPooling3D, ZeroPadding3D, AveragePooling3D

from keras.optimizers import SGD
from keras.optimizers import *
from keras.layers.merge import *
from keras.layers.normalization import BatchNormalization
from keras.layers.noise import GaussianNoise
import keras.models as models
from keras.layers import ZeroPadding2D,Deconvolution2D,Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout
from keras.layers.core import Layer,Dense, Dropout, Activation, Flatten, Reshape, Permute
from keras.layers.convolutional import *
from keras.layers.normalization import BatchNormalization
from keras.utils import np_utils
from keras.utils import plot_model
from keras.utils.io_utils import HDF5Matrix
from keras.constraints import maxnorm
from keras.objectives import binary_crossentropy
from keras import backend as K
from functools import partial

import keras_resnet.models

from tensorflow.python.client import device_lib

import tensorflow as tf

# Keras sequence
from keras.utils import Sequence

class ImageSequence(Sequence):

    def __init__ ( self, dataPath, subjects, subjectClass, batchSize, useClasses=True ):
        """
        Create a sequence to return image data and segmentation (useClasse=False),
        or classes (useClasses=True).  When using classes, will return two values (not a binary
        indicator as in previous versions).

        Returns `X` as a (batchSize, 256,256,2) image (T1 and T2).
        """
        self.logger = logging.getLogger("BlockGenerator")
        self.batchSize = batchSize
        self.subjectClass = subjectClass
        self.useClasses = useClasses
        # Find all our T1
        selected = []
        class_map = {}
        for fn in glob.glob( os.path.join ( dataPath, "*T1*.gz")):
          for match in subjects:
            if os.path.basename(fn).startswith ( match ):
              selected.append ( fn )
              class_map[fn] = subjectClass[match]["class"]
              break
        selected = sorted(selected)
        self.t1Filenames = selected
        self.t2Filenames = [ x.replace ( 'T1', 'T2') for x in selected]
        self.segmentationFilenames = [ x.replace ( 'T1', 'Segmentation') for x in selected]
        self.logger.info ( "Found {} files in {}".format ( len(self.t1Filenames), dataPath ))
        c = [ class_map[x] for x in self.t1Filenames]
        self.classes = np.array ( c )

    def __len__ ( self ):
        l = math.ceil(len(self.t1Filenames) / self.batchSize)
        self.logger.debug ( "Length of BlockGenerator is {}".format(l))
        return l

    def __getitem__ (self, idx):
        """
        Return batch for idx
        """
        self.logger.debug ( "Getting {}".format ( idx ))
        fn = self.t1Filenames[idx * self.batchSize: (idx+1)*self.batchSize]
        fn2 = self.t2Filenames[idx * self.batchSize: (idx+1)*self.batchSize]
        sfn = self.segmentationFilenames[idx * self.batchSize: (idx+1)*self.batchSize]
        c = self.classes[idx * self.batchSize: (idx+1)*self.batchSize]
        
        t1 = np.array ( [ nib.load(filename).get_data() for filename in fn ] )
        t2 = np.array ( [ nib.load(filename).get_data() for filename in fn2 ] )
        self.logger.debug ( 't1 shape is {}'.format ( t1.shape ))
        batchX = np.stack ( (t1,t2), axis=-1)
        self.logger.debug ( "batchX shape is {}".format ( batchX.shape ))
        
        if self.useClasses:
            batchY = np.stack ( (1-c, c), axis=1)
        else:
            batchY = np.array ( [ np.expand_dims ( nib.load(filename).get_data() ) for filename in sfn] )
        return batchX, batchY



def makeBlockModel(args):
  K.set_image_data_format('channels_last')

  # resnet
  x = keras.layers.Input ( (256,256,2) )
  if args.model.lower() == 'resnet50':
    return keras_resnet.models.ResNet50 ( x, classes=2 )
  if args.model.lower() == 'resnet18':
    return keras_resnet.models.ResNet18 ( x, classes=2 )
  if args.model.lower() == 'resnet34':
    return keras_resnet.models.ResNet34 ( x, classes=2 )
  if args.model.lower() == 'resnet50':
    return keras_resnet.models.ResNet50 ( x, classes=2 )
  if args.model.lower() == 'resnet101':
    return keras_resnet.models.ResNet101 ( x, classes=2 )
  if args.model.lower() == 'resnet152':
    return keras_resnet.models.ResNet152 ( x, classes=2 )
  if args.model.lower() == 'resnet200':
    return keras_resnet.models.ResNet200 ( x, classes=2 )

  
  model = Sequential()
  # 1st layer group
  model.add(Convolution2D(16, (3, 3), activation='relu',
                          padding='same', name='conv1',
                          input_shape=(256,256,2)))
  model.add(MaxPooling2D(pool_size=(2,2),
                         border_mode='valid', name='pool1'))
  # 2nd layer group
  model.add(Convolution2D(32, (3, 3), activation='relu',
                          border_mode='same', name='conv2'))
  model.add(MaxPooling2D(pool_size=(2,2),
                         border_mode='valid', name='pool2'))
  # 2nd layer group
  model.add(Convolution2D(64, (3, 3), activation='relu',
                          border_mode='same', name='conv3'))
  model.add(MaxPooling2D(pool_size=(2,2),
                         border_mode='valid', name='pool3'))
  # 2nd layer group
  model.add(Convolution2D(96, (3, 3), activation='relu',
                          border_mode='same', name='conv4'))
  model.add(MaxPooling2D(pool_size=(2,2),
                         border_mode='valid', name='pool4'))
  
  model.add(Convolution2D(128, (3, 3), activation='relu',
                          border_mode='same', name='conv5a'))
  model.add(Convolution2D(160, (3, 3), activation='relu',
                          border_mode='same', name='conv5b'))
  # model.add(ZeroPadding2D(padding=(0, 1, 1)))
  model.add(MaxPooling2D(pool_size=(2,2),
                         border_mode='valid', name='pool5'))
  model.add(Flatten()) # Maybe should replace with GlobalAveragePooling or even better GlobalMaxPooling2D  x = GlobalMaxPooling2D()(x)
  # FC layers group
  model.add(Dense(64, activation='relu', name='fc6'))
  model.add(Dropout(.25))
  model.add(Dense(32, activation='relu', name='fc7'))
  model.add(Dropout(.25))
  model.add(Dense(2, activation='sigmoid', name='fc8'))
  return model
  

def buildModel(args):
  with open(args.subjects, 'r') as f:
    subjects = json.load(f)

  model = makeBlockModel(args)
  print(model.summary())

  lrate = 0.001
  model.compile(optimizer=SGD(lr=lrate), loss=binary_crossentropy, metrics=['accuracy'])

  if args.model_image:
    plot_model(model,to_file=args.model_image, show_shapes=True)

  modelCheckpoint = ModelCheckpoint( args.weightsFilename, verbose=1, monitor='loss',save_best_only=True)
  earlyStoppingCallback = EarlyStopping(monitor='loss', patience=15)
  callbacks = [earlyStoppingCallback]
  if args.tensorboard:
    tensorboardCallback = TensorBoard(log_dir=args.tensorboard)
    callbacks.append ( tensorboardCallback )

  trainSequence = ImageSequence ( args.dataPath, subjects['train'], subjects['subjects'], args.batch_size )
  testSequence = ImageSequence ( args.dataPath, subjects['test'], subjects['subjects'], args.batch_size )
  validationSequence = ImageSequence ( args.dataPath, subjects['validate'], subjects['subjects'], args.batch_size )

  history = model.fit_generator(
    trainSequence,
    steps_per_epoch=len(trainSequence),
    validation_data = validationSequence,
    validation_steps = len(validationSequence),
    epochs=args.epochs,
    use_multiprocessing=True,
    workers=args.workers,
    verbose=1,
    shuffle=True,
    callbacks=callbacks
  )

  loss = model.evaluate_generator (
    testSequence,
    steps=len(testSequence),
    workers=args.workers,
    use_multiprocessing=True
  )
  logger.info ( "test loss: {}".format(loss))
  
  model.save ( args.weightsFilename )

if __name__ == "__main__":
  masterParser = argparse.ArgumentParser( description='machine learning for LGG-GBM dataset')
  masterParser.add_argument ( '--debug', help="Debug output", action='store_true')
  parser = masterParser.add_subparsers()
  
  subParser = parser.add_parser("train", help="train the model")
  subParser.set_defaults ( function=buildModel )
  subParser.add_argument ( "--batch-size",  help="batch size for training", required=False, type=int,  default=16 )
  subParser.add_argument ( "--epochs",  help="number of epochs to run", required=False, type=int,  default=300 )
  subParser.add_argument ( "--workers",  help="number background workers", required=False, type=int,  default=8 )
  subParser.add_argument ( "--tensorboard", help="directory for tensor board" )
  subParser.add_argument ( "--model-image", help="save a PNG of the model" )
  subParser.add_argument ( "--model", help="model type, must be resnet18, resnet34, resnet50, resnet101, resnet152, resnet200", default=None )
  subParser.add_argument ( "dataPath", help="path to directory data and segmentations")
  subParser.add_argument ( "subjects", help="JSON file with subject data, generated using preprocess")
  subParser.add_argument ( "weightsFilename", help="save weights to this file in HDF5 format")

  logging.basicConfig(format="%(asctime)s - %(name)s - %(levelname)s - %(message)s")
  args = masterParser.parse_args()
  logging.getLogger().setLevel ( logging.INFO )
  if args.debug:
    logging.getLogger().setLevel ( logging.DEBUG )
  
  args.function ( args )

